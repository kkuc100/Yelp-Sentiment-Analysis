% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{liu2019roberta,
  author    = {Liu, Y. and Ott, M. and Goyal, N. and Du, J. and Joshi, M. and Chen, D. and Levy, O. and Lewis, M. and Zettlemoyer, L. and Stoyanov, V.},
  title     = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  journal   = {ArXiv},
  year      = {2019},
  volume    = {abs/1907.11692},
  url       = {https://arxiv.org/abs/1907.11692}
}

@inproceedings{devlin2019bert,
  author    = {Devlin, J. and Chang, M.-W. and Lee, K. and Toutanova, K.},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages     = {4171--4186},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics}
}

@article{chung2022scaling,
  author    = {Chung, H. W. and Hou, L. and Longpre, S. and Zoph, B. and Tay, Y. and Fedus, W. and Li, E. and Wang, X. and Dehghani, M. and Brahma, S. and Webson, A. and Gu, S. S. and Dai, Z. and Suzgun, M. and Chen, X. and Chowdhery, A. and Valter, D. and Narang, S. and Mishra, G. and Yu, A. W. and Zhao, V. and Huang, Y. and Dai, A. M. and Yu, H. and Petrov, S. and Chi, E. H. and Dean, J. and Devlin, J. and Roberts, A. and Zhou, D. and Le, Q. V. and Wei, J.},
  title     = {Scaling Instruction-Finetuned Language Models},
  journal   = {ArXiv},
  year      = {2022},
  volume    = {abs/2210.11416},
  url       = {https://arxiv.org/abs/2210.11416}
}

@misc{huggingface_yelp,
  author    = {{Hugging Face}},
  title     = {Yelp/Yelp\_review\_full · Datasets at Hugging Face},
  year      = {2024},
  note      = {Accessed 13 June 2024},
  url       = {https://huggingface.co/datasets/Yelp/yelp_review_full}
}

@article{opitz1999popular,
  author    = {Opitz, D. and Maclin, R.},
  title     = {Popular ensemble methods: An empirical study},
  journal   = {Journal of Artificial Intelligence Research},
  year      = {1999},
  volume    = {11},
  pages     = {169--198},
  url       = {https://doi.org/10.1613/jair.614}
}

@article{schapire1990strength,
  author    = {Schapire, R. E.},
  title     = {The strength of weak learnability},
  journal   = {Machine Learning},
  year      = {1990},
  volume    = {5},
  pages     = {197--227},
  url       = {https://doi.org/10.1007/BF00116037}
}

@article{he2020deberta,
  author    = {He, P. and Liu, X. and Gao, J. and Chen, W.},
  title     = {DeBERTa: Decoding-enhanced BERT with disentangled attention},
  journal   = {arXiv preprint},
  year      = {2020},
  volume    = {arXiv:2006.03654},
  url       = {https://arxiv.org/abs/2006.03654}
}

@inproceedings{hu2004mining,
  author    = {Hu, M. and Liu, B.},
  title     = {Mining and summarizing customer reviews},
  booktitle = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages     = {168--177},
  year      = {2004},
  publisher = {ACM},
  url       = {https://doi.org/10.1145/1014052.1014073}
}

@book{liu2012sentiment,
  author    = {Liu, B.},
  title     = {Sentiment Analysis and Opinion Mining},
  series    = {Synthesis Lectures on Human Language Technologies},
  volume    = {5},
  pages     = {1--167},
  year      = {2012},
  publisher = {Morgan \& Claypool Publishers},
  url       = {https://doi.org/10.2200/S00416ED1V01Y201204HLT016}
}

@inproceedings{pang2004sentimental,
  author    = {Pang, B. and Lee, L.},
  title     = {A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts},
  booktitle = {Proceedings of the ACL},
  pages     = {271--278},
  year      = {2004},
  publisher = {ACL},
  url       = {https://doi.org/10.3115/1218955.1218990}
}

@inproceedings{pang2002thumbs,
  author    = {Pang, B. and Lee, L. and Vaithyanathan, S.},
  title     = {Thumbs up?: Sentiment classification using machine learning techniques},
  booktitle = {Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10},
  pages     = {79--86},
  year      = {2002},
  publisher = {ACL},
  url       = {https://doi.org/10.3115/1118693.1118704}
}

@inproceedings{rish2001empirical,
  author    = {Rish, I.},
  title     = {An empirical study of the Naïve Bayes classifier},
  booktitle = {IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence},
  pages     = {3},
  year      = {2001}
}

@misc{acl2023ethics,
  author = {Association for Computational Linguistics},
  title = {ACL Ethics Policy},
  howpublished = {\url{https://www.aclweb.org/anthology/2023.acl-ethics/}},
  note = {Accessed: 2024-07-27},
  year = {2023}
}

@article{luca2016reviews,
  author = {Luca, Michael},
  title = {Reviews, Reputation, and Revenue: The Case of Yelp.com},
  journal = {Harvard Business School Working Paper},
  year = {2016},
  url = {https://www.hbs.edu/faculty/Pages/item.aspx?num=49188}
}

@article{lee2014impact,
  author = {Lee, Joon and Shin, Doo-Hyung},
  title = {The impact of online consumer reviews on the credibility of a restaurant's reputation},
  journal = {International Journal of Hospitality Management},
  volume = {36},
  pages = {230--239},
  year = {2014},
  doi = {10.1016/j.ijhm.2013.09.003}
}

@book{jurafsky2020speech,
  author = {Jurafsky, Daniel and Martin, James H.},
  title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  publisher = {Prentice Hall},
  year = {2020},
  url = {https://web.stanford.edu/~jurafsky/slp3/}
}

@book{manning1999foundations,
  author = {Manning, Christopher D. and Schütze, Hinrich},
  title = {Foundations of Statistical Natural Language Processing},
  publisher = {MIT Press},
  year = {1999},
  url = {https://www.cs.stanford.edu/~manning/cs124/lec/embeddings.pdf}
}

@article{pang2008sentiment,
  author = {Pang, Bo and Lee, Lillian},
  title = {Sentiment Analysis and Opinion Mining},
  journal = {Foundations and Trends in Information Retrieval},
  volume = {2},
  number = {1--2},
  pages = {1--135},
  year = {2008},
  doi = {10.1561/1500000011}
}

@inproceedings{zhang2015sensitivity,
  author = {Zhang, Xiang and Wallace, Brian C.},
  title = {A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages = {253--263},
  year = {2015},
  url = {https://www.aclweb.org/anthology/D15-1020/}
}

@report{dankhara2022,
  author = {Dhruvin Dankhara},
  title = {A Review of Sentiment Analysis of Tweets},
  institution = {School of Engineering, University of Guelph},
  year = {2022},
  month = {April},
  address = {Guelph, Canada},
  note = {MEng Thesis},
  doi = {10.13140/RG.2.2.14283.67366/1},
  url = {https://www.researchgate.net/publication/360076165_A_Review_of_Sentiment_Analysis_of_Tweets}
}

@article{kokab2022transformer,
  author = {Sayyida Tabinda Kokab and Sohail Asghar and Shehneela Naz},
  title = {Transformer-based deep learning models for the sentiment analysis of social media data},
  journal = {Array},
  volume = {14},
  pages = {100157},
  year = {2022},
  issn = {2590-0056},
  doi = {10.1016/j.array.2022.100157},
  url = {https://www.sciencedirect.com/science/article/pii/S2590005622000224},
  keywords = {Sentiment analysis, Social media, Deep learning, BERT, CNN, LSTM},
  abstract = {Sentiment analysis (SA) is a widely used contextual mining technique for extracting useful and subjective information from text-based data. It applies on Natural Language Processing (NLP), text analysis, biometrics, and computational linguistics to identify, analyse, and extract responses, states, or emotions from the data. The features analysis technique plays a significant role in the development and improvement of a SA model. Recently, GloVe and Word2vec embedding models have been widely used for feature extractions. However, they overlook sentimental and contextual information of the text and need a large corpus of text data for training and generating exact vectors. These techniques generate vectors for just those words that are included in their vocabulary and ignore Out of Vocabulary Words (OOV), which can lead to information loss. Another challenge for the classification of sentiments is that of the lack of readily available annotated data. Sometimes, there is a contradiction between the review and their label that may cause misclassification. The aim of this paper is to propose a generalized SA model that can handle noisy data, OOV words, sentimental and contextual loss of reviews data. In this research, an effective Bi-directional Encoder Representation from Transformers (BERT) based Convolution Bi-directional Recurrent Neural Network (CBRNN) model is proposed with for exploring the syntactic and semantic information along with the sentimental and contextual analysis of the data. Initially, the zero-shot classification is used for labelling the reviews by calculating their polarity scores. After that, a pre-trained BERT model is employed for obtaining sentence-level semantics and contextual features from that data and generate embeddings. The obtained contextual embedded vectors were then passed to the neural network, comprised of dilated convolution and Bi-LSTM. The proposed model uses dilated convolution instead of classical convolution to extract local and global contextual semantic features from the embedded data. Bi-directional Long Short-Term Memory (Bi-LSTM) is used for the entire sequencing of the sentences. The CBRNN model is evaluated across four diverse domain text datasets based on accuracy, precision, recall, f1-score and AUC values. Thus, CBRNN can be efficiently used for performing SA tasks on social media reviews, without any information loss.}
}

